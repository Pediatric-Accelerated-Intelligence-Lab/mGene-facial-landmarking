{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtmrAnIOzLSm"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import imutils\n",
    "import matplotlib.image as mpimg\n",
    "from collections import OrderedDict\n",
    "from skimage import io, transform\n",
    "from math import *\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(0) #setting a seed for same randomness every time\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwkIZZp5x5jO"
   },
   "outputs": [],
   "source": [
    "def getLandmarksFromTxt(txt):\n",
    "    return np.loadtxt(txt, delimiter = \",\") # 44x2 numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all the landmarking files and loading them into Python - 11 minutes to run\n",
    "\n",
    "rootdir = \"C:/../Data\"\n",
    "\n",
    "# List all files in the folder - load images, landmarks, bounding box around face\n",
    "img_dir = os.path.join(rootdir, \"imgs\")\n",
    "img_file_list = sorted(os.listdir(img_dir))\n",
    "\n",
    "lm_dir = os.path.join(rootdir, \"lms\")\n",
    "lm_file_list = sorted(os.listdir(lm_dir))\n",
    "\n",
    "bbox_dir = os.path.join(rootdir, \"bbox\")\n",
    "bbox_file_list = sorted(os.listdir(bbox_dir))\n",
    "\n",
    "num_files = np.shape(img_file_list)[0]\n",
    "print(num_files)\n",
    "\n",
    "ids = [] #list of strings\n",
    "lms = [] #list of numpy arrays\n",
    "imgs =[] # list of numpy arrays\n",
    "imgfiles = [] # list of image file paths\n",
    "crops = [] # list of dictionaries containing cropping details (set to full image right now)\n",
    "\n",
    "for i in range(num_files): \n",
    "    \n",
    "    #load image\n",
    "    img_file = rootdir + \"/imgs/\" + img_file_list[i]\n",
    "    img = Image.open(img_file) \n",
    "    imgarr = np.asarray(img)\n",
    "    imgfiles.append(img_file)\n",
    "    imgs.append(imgarr)\n",
    "\n",
    "    #load lm\n",
    "    lm_file = rootdir + \"/lms/\" + lm_file_list[i]\n",
    "    lm = getLandmarksFromTxt(lm_file)\n",
    "    lms.append(lm)\n",
    "\n",
    "    #generate crop\n",
    "    margin = 10\n",
    "    allx = lm[:,0] \n",
    "    ally = lm[:,1] \n",
    "    sx = min(allx)-margin\n",
    "    sy = min(ally)-margin\n",
    "    lx = max(allx)+margin\n",
    "    ly = max(ally)+margin\n",
    "    w = lx-sx\n",
    "    h = ly-sy\n",
    "    cropsDict = {'top': str(int(sy)), 'left' : str(int(sx)), 'width': str(int(w)), 'height' : str(int(h))}\n",
    "    crops.append(cropsDict)\n",
    "\n",
    "    #save image id\n",
    "    id = img_file[0:5] # e.g 09524\n",
    "    ids.append(id)\n",
    "\n",
    "\n",
    "\n",
    "# creating a database\n",
    "data = list(zip(ids, imgs, lms, imgfiles, crops))\n",
    "df = pd.DataFrame(data, columns=['ID', 'Face Image', 'Landmarks', 'Image Files', 'Crops'])\n",
    "df        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMMyYVcOjSw0"
   },
   "source": [
    "## Visualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "De4w_z-uz6vT",
    "outputId": "3a4ac38c-7fd1-4c34-ac23-12149867e2c0"
   },
   "outputs": [],
   "source": [
    "imgnum = 5\n",
    "\n",
    "landmarks = df['Landmarks'][imgnum]\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(mpimg.imread(df['Image Files'][imgnum]))\n",
    "plt.scatter(landmarks[:,0], landmarks[:,1], s = 3, c = 'y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvCbi2UYkKmr"
   },
   "source": [
    "## Create dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CjKNGpx1XpAK"
   },
   "outputs": [],
   "source": [
    "class Transforms():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def rotate(self, image, landmarks, angle): # not used.\n",
    "        angle = random.uniform(-angle, +angle)\n",
    "\n",
    "        transformation_matrix = torch.tensor([\n",
    "            [+cos(radians(angle)), -sin(radians(angle))],\n",
    "            [+sin(radians(angle)), +cos(radians(angle))]\n",
    "        ])\n",
    "\n",
    "        image = imutils.rotate(np.array(image), angle)\n",
    "\n",
    "        #uncomment the two lines below IF you are centering landmarks between -0.5 and 0.5\n",
    "\n",
    "        #landmarks = landmarks - 0.5  \n",
    "        new_landmarks = np.matmul(landmarks, transformation_matrix)\n",
    "        #new_landmarks = new_landmarks + 0.5\n",
    "        return Image.fromarray(image), new_landmarks\n",
    "\n",
    "    def resize(self, image, landmarks, img_size):\n",
    "        image = TF.resize(image, img_size)\n",
    "        return image, landmarks\n",
    "\n",
    "    def color_jitter(self, image, landmarks):\n",
    "        color_jitter = transforms.ColorJitter(brightness=0.3,\n",
    "                                              contrast=0.3,\n",
    "                                              saturation=0.3,\n",
    "                                              hue=0.1)\n",
    "        image = color_jitter(image)\n",
    "        return image, landmarks\n",
    "\n",
    "    def crop_face(self, image, landmarks, crops):\n",
    "        left = int(crops['left'])\n",
    "        top = int(crops['top'])\n",
    "        width = int(crops['width'])\n",
    "        height = int(crops['height'])\n",
    "\n",
    "        image = TF.crop(image, top, left, height, width)\n",
    "\n",
    "        img_shape = np.array(image).shape\n",
    "        landmarks = torch.tensor(landmarks) - torch.tensor([[left, top]])\n",
    "        landmarks = landmarks / torch.tensor([img_shape[1], img_shape[0]]) #normalizing the landmarks to be between 0 and 1\n",
    "        return image, landmarks\n",
    "\n",
    "    def __call__(self, image, landmarks, crops):\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        image, landmarks = self.crop_face(image, landmarks, crops) # this is where landmarks also change format\n",
    "        image, landmarks = self.resize(image, landmarks, (224, 224))\n",
    "        image, landmarks = self.color_jitter(image, landmarks)\n",
    "        #image, landmarks = self.rotate(image, landmarks, angle=10)\n",
    "\n",
    "        image = TF.to_tensor(image)\n",
    "        image = TF.normalize(image, [0.5], [0.5])\n",
    "        return image, landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ux92wKUslw-9"
   },
   "outputs": [],
   "source": [
    "class FaceLandmarksDataset(Dataset):\n",
    "\n",
    "    def __init__(self, transform=None, database=None):\n",
    "\n",
    "        self.image_filenames = database['Image Files'].values\n",
    "        \n",
    "        self.landmarks = []\n",
    "        self.crops = []\n",
    "        self.transform = transform\n",
    "        self.landmarks = database['Landmarks'].values \n",
    "        \n",
    "        self.crops = database['Crops'].values\n",
    "\n",
    "        #print(len(self.image_filenames) ,  len(self.landmarks))\n",
    "        assert len(self.image_filenames) == len(self.landmarks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.image_filenames[index], 0)\n",
    "        landmarks = self.landmarks[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image, landmarks = self.transform(image, landmarks, self.crops[index])\n",
    "        \n",
    "        landmarks = landmarks - 0.5 #centering the data between -0.5 and 0.5\n",
    "        return image, landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FaceLandmarksDataset(transform = Transforms(), database=df)\n",
    "print(np.shape(dataset[3][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgKfWZ22A5lr"
   },
   "source": [
    "## Visualize Train Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "id": "0kJj8CL9ppPF",
    "outputId": "943dabde-2349-4da6-fa9e-1f6b667e0d53"
   },
   "outputs": [],
   "source": [
    "image, landmarks = dataset[16] #choose an image/lm set\n",
    "\n",
    "#landmarks = (landmarks) * 224\n",
    "landmarks = (landmarks + 0.5) * 224\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(image.numpy().squeeze(), cmap='gray');\n",
    "plt.scatter(landmarks[:,0], landmarks[:,1], s=3, c='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJccVxoGAmnF"
   },
   "source": [
    "## Split the dataset into train and valid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "eBAnZv_3KuuJ",
    "outputId": "e02cb273-81d4-46ca-cf1c-aba1ceec926f"
   },
   "outputs": [],
   "source": [
    "# split the dataset into validation and test sets\n",
    "len_valid_set = int(0.1*len(dataset))\n",
    "len_test_set = int(0.1*len(dataset))\n",
    "len_train_set = len(dataset) - len_valid_set - len_test_set\n",
    "\n",
    "print(\"The length of Train set is {}\".format(len_train_set))\n",
    "print(\"The length of Valid set is {}\".format(len_valid_set))\n",
    "print(\"The length of Test set is {}\".format(len_test_set))\n",
    "\n",
    "torch.manual_seed(0) #setting a seed for same randomness every time\n",
    "train_dataset , valid_dataset, test_dataset = torch.utils.data.random_split(dataset , [len_train_set, len_valid_set, len_test_set])\n",
    "\n",
    "# shuffle and batch the datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0) #OG num_workers=4 runs an infinite loop\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=8, shuffle=True, num_workers=0) #OG num_workers=4\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=True, num_workers=0) #OG num_workers=4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQH-0jKP-e5n"
   },
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igwHkSVtD1Tf"
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,num_classes=88):\n",
    "        super().__init__()\n",
    "        self.model_name='resnet18'\n",
    "        self.model=models.resnet18()\n",
    "        self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.fc=nn.Linear(self.model.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anXScjUBughk"
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dNBMR_zuK9w"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def print_overwrite(step, total_step, loss, operation):\n",
    "    sys.stdout.write('\\r')\n",
    "    if operation == 'train':\n",
    "        sys.stdout.write(\"Train Steps: %d/%d  Loss: %.4f \" % (step, total_step, loss))\n",
    "    else:\n",
    "        sys.stdout.write(\"Valid Steps: %d/%d  Loss: %.4f \" % (step, total_step, loss))\n",
    "\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lVFet1GQHvx"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "#network = Network()\n",
    "#network.cuda()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "network = Network().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.0001)\n",
    "\n",
    "loss_min = np.inf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GL3el20OEZG8",
    "outputId": "218ed4d3-e442-4197-93e8-7d0df9af2e10"
   },
   "outputs": [],
   "source": [
    "# start a PYTORCH TRAINING ------------\n",
    "\n",
    "model_path = 'C:/../myModel.pth'\n",
    "start_time = time.time()\n",
    "num_epochs = 100\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    print(\"entering epoch: \", epoch)\n",
    "    loss_train = 0\n",
    "    loss_valid = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    network.train()\n",
    "    for step in range(1,len(train_loader)+1):\n",
    "\n",
    "        images, landmarks = next(iter(train_loader))\n",
    "\n",
    "        #print(landmarks.shape)\n",
    "        \n",
    "        images = images.cuda()\n",
    "        landmarks = landmarks.view(landmarks.size(0),-1).cuda()\n",
    "\n",
    "        predictions = network(images)\n",
    "        #print(type(predictions))\n",
    "        predictions = predictions.to(torch.float64)\n",
    "        #print(predictions.dtype)\n",
    "\n",
    "        # clear all the gradients before calculating them\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # find the loss for the current step\n",
    "        loss_train_step = criterion(predictions, landmarks)\n",
    "\n",
    "        # calculate the gradients\n",
    "        loss_train_step.backward()\n",
    "\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train += loss_train_step.item()\n",
    "        running_loss = loss_train/step\n",
    "\n",
    "        print_overwrite(step, len(train_loader), running_loss, 'train')\n",
    "\n",
    "        epoch_len = len(train_loader)\n",
    "        writer.add_scalar(\"train_loss\", loss_train_step.item(), epoch_len * epoch + step)\n",
    "    \n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for step in range(1,len(valid_loader)+1):\n",
    "\n",
    "            images, landmarks = next(iter(valid_loader))\n",
    "\n",
    "            images = images.cuda()\n",
    "            landmarks = landmarks.view(landmarks.size(0),-1).cuda()\n",
    "\n",
    "            predictions = network(images)\n",
    "\n",
    "            # find the loss for the current step\n",
    "            loss_valid_step = criterion(predictions, landmarks)\n",
    "\n",
    "            loss_valid += loss_valid_step.item()\n",
    "            running_loss = loss_valid/step\n",
    "\n",
    "            print_overwrite(step, len(valid_loader), running_loss, 'valid')\n",
    "\n",
    "    loss_train /= len(train_loader)\n",
    "    loss_valid /= len(valid_loader)\n",
    "\n",
    "    #print('\\n--------------------------------------------------')\n",
    "    print('Epoch: {}  Train Loss: {:.4f}  Valid Loss: {:.4f}'.format(epoch, loss_train, loss_valid))\n",
    "    #print('--------------------------------------------------')\n",
    "\n",
    "    if loss_valid < loss_min:\n",
    "        loss_min = loss_valid\n",
    "        torch.save(network.state_dict(), model_path)\n",
    "        print(\"\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\".format(loss_min, epoch, num_epochs))\n",
    "        print('Model Saved\\n')\n",
    "    \n",
    "    writer.add_scalar(\"val_mean_dice\", loss_valid, epoch + 1)\n",
    "\n",
    "writer.close()\n",
    "print('Training Complete')\n",
    "print(\"Total Elapsed Time : {} s\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOMxVtnNUsis"
   },
   "source": [
    "## Predict on test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Euclidean distance between two points\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.linalg.norm(point1 - point2)\n",
    "\n",
    "# Function to calculate Mean Euclidean Distance Error and Normalized Mean Error\n",
    "def calculate_errors(ground_truth_landmarks, predicted_landmarks):\n",
    "    # ground_truth_landmarks is (704, 44, 2)\n",
    "    num_images = len(ground_truth_landmarks)\n",
    "    num_landmarks = len(ground_truth_landmarks[0])  # Assuming the same number of landmarks for all images\n",
    "\n",
    "    MED = []\n",
    "    NME_IP = []\n",
    "    NME_IO = []\n",
    "    MNE = []\n",
    "\n",
    "    for i in range(num_images):\n",
    "        \n",
    "        interpupilar_distance = euclidean_distance(ground_truth_landmarks[i][4], ground_truth_landmarks[i][9]) #5th & 10th landmarks are eye pupils\n",
    "        interoccular_distance = euclidean_distance(ground_truth_landmarks[i][0], ground_truth_landmarks[i][7]) #1st & 8th landmarks are outer corners of the eye\n",
    "\n",
    "        #print(\"eye pupil locations: \", ground_truth_landmarks[i][4], ground_truth_landmarks[i][9])\n",
    "        #print(\"ID = \", interpupilar_distance)\n",
    "        \n",
    "        ed_img = 0\n",
    "        ed_img_normIP = 0\n",
    "        ed_img_normIO = 0\n",
    "        norm_error = 0\n",
    "        fr = 0\n",
    "\n",
    "        for j in range(num_landmarks):\n",
    "            ed_lm = euclidean_distance(predicted_landmarks[i][j], ground_truth_landmarks[i][j])\n",
    "            ed_lm_normIP = ed_lm / interpupilar_distance\n",
    "            ed_lm_normIO = ed_lm / interoccular_distance\n",
    "            ed_lm_normBB = ed_lm / 224 #bounding box size #Grooby2023\n",
    "            \n",
    "            ed_img += ed_lm\n",
    "            ed_img_normIP += ed_lm_normIP\n",
    "            ed_img_normIO += ed_lm_normIO\n",
    "            norm_error += ed_lm_normBB\n",
    "        \n",
    "        #print(\"Mean Euclidean Distance (MED) error of image = \", ed_img/num_landmarks)\n",
    "        #print(\"Normalized Mean Error (NME) of image = \", ed_img_norm/num_landmarks)\n",
    "        #print()\n",
    "        \n",
    "        MED.append(round(ed_img/num_landmarks, 3))\n",
    "        NME_IP.append (round(ed_img_normIP/num_landmarks, 3))\n",
    "        NME_IO.append (round(ed_img_normIO/num_landmarks, 3))\n",
    "        MNE.append (round(norm_error/num_landmarks, 3))\n",
    "\n",
    "    return MED,NME_IP,NME_IO,MNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating predicted landmarks in test set\n",
    "model_path = model_path\n",
    "\n",
    "ground_truth_landmarks_list = []\n",
    "predicted_landmarks_list = []\n",
    "i=0\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    best_network = Network()\n",
    "    best_network.cuda()\n",
    "    best_network.load_state_dict(torch.load(model_path))\n",
    "    best_network.eval()\n",
    "\n",
    "    x = len(test_loader)+1   ## Think multiples of 8 (x=3 will display 2*8=16 images)\n",
    "\n",
    "    for step in range(1,x):\n",
    "\n",
    "        images, landmarks = next(iter(test_loader))\n",
    "        \n",
    "        images = images.cuda()\n",
    "        #landmarks = (landmarks) * 224 #8x44x2\n",
    "        landmarks = (landmarks + 0.5) * 224 #8x44x2\n",
    "\n",
    "        #predictions = (best_network(images).cpu() ) * 224\n",
    "        predictions = (best_network(images).cpu() + 0.5) * 224\n",
    "        predictions = predictions.view(-1,44,2) #8x44x2\n",
    "\n",
    "        for img_num in range(len(landmarks)):\n",
    "            ground_truth_landmarks_list.append( np.array(landmarks[img_num,:,:]) ) \n",
    "            predicted_landmarks_list.append( np.array(predictions[img_num,:,:]) )\n",
    "            i = i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(ground_truth_landmarks_list))\n",
    "print(np.shape(predicted_landmarks_list))\n",
    "\n",
    "# print(ground_truth_landmarks_list[13][0:5][:])\n",
    "# print(predicted_landmarks_list[13][0:5][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Total number of test images: {}'.format(len(test_dataset)))\n",
    "#print(\"Total images processed = \",i, \"\\n\")\n",
    "\n",
    "med, nmeIP, nmeIO, mne = calculate_errors(ground_truth_landmarks_list, predicted_landmarks_list)\n",
    "\n",
    "print(\"Number of images processed = \", len(nmeIP))\n",
    "print(f\"Mean Euclidean Distance Error (MED) per image: {med}\")\n",
    "print(f\"Normalized Mean Error (NME) normalized by IPD per image: {nmeIP}\")\n",
    "print(f\"Normalized Mean Error (NME) normalized by IOD per image: {nmeIO}\")\n",
    "print(f\"Mean Normalized Error (MNE) normalized by Bounding Box per image: {mne}\")\n",
    "\n",
    "print(f\"Average MED: {round(np.mean(med), 3)}\")\n",
    "print(f\"Average NME_IP: {round(np.mean(nmeIP), 3)}\")\n",
    "print(f\"Average NME_IO: {round(np.mean(nmeIO), 3)}\")\n",
    "print(f\"Average MNE: {round(np.mean(mne), 3)}\")\n",
    "\n",
    "all_threshold = [0.05, 0.10, 0.15]\n",
    "FR = []\n",
    "print(\"Failure rate for the given thresholds is:\")\n",
    "for threshold in all_threshold:\n",
    "    fails = [(val>threshold) for val in nmeIO ]\n",
    "    FR.append(( fails.count(True) / len(nmeIP) )*100)\n",
    "print(np.round(FR,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving average results to a text file\n",
    "# \n",
    "fname = os.path.basename(model_path)[:-4] + \".txt\"\n",
    "file = open(fname,'a+')\n",
    "print(\"Number of images processed = \", len(nmeIP), file=file)\n",
    "print(f\"Average MED: {round(np.mean(med), 3)}\", file=file)\n",
    "print(f\"Average NME_IP: {round(np.mean(nmeIP), 3)}\", file=file)\n",
    "print(f\"Average NME_IO: {round(np.mean(nmeIO), 3)}\", file=file)\n",
    "print(f\"Average MNE: {round(np.mean(mne), 3)}\", file=file)\n",
    "print(\"Threshold[5,10,15] using percentage NMEIO : \", np.round(FR,3), file=file)\n",
    "file.close() # close the file ones the function execution completes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Write per-image results to a file\n",
    "\n",
    "fname = os.path.basename(model_path)[:-4] + \".xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(fname) as writer:\n",
    "    x = np.concatenate((np.expand_dims(np.array(med), 1) , np.expand_dims(np.array(nmeIP), 1), np.expand_dims(np.array(nmeIO), 1), np.expand_dims(np.array(mne), 1)), axis=1)\n",
    "    x = pd.DataFrame(x, columns=['MED', 'NME_IP', 'NME_IO', 'MNE'])\n",
    "    x.to_excel(writer)#, sheet_name= \"p\"+str(i+1), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
